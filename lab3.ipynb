{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 3: Markov decision problems\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;.\n",
    "\n",
    "### 1. Modeling\n",
    "\n",
    "Consider once again the knight domain described in the Homework and which you described as a Markov decision process.\n",
    "\n",
    "<img src=\"knight.png\" width=\"200px\">\n",
    "\n",
    "Recall that:\n",
    "\n",
    "* At each step, the knight may move in any of the four directions---up, down, left and right. \n",
    "\n",
    "* The movement succeeds with a 0.6 probability and fails with a 0.4 probability. When the movement fails, the knight may stay in the same cell or move to one of the immediately adjacent cells (if there is one) with equal probability.\n",
    "\n",
    "* The goal of the knight is to save (reach) the princess and avoid the dragon.\n",
    "\n",
    "**Throughout the lab, use $\\gamma=0.99$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Implement your Markov decision process in Python. In particular,\n",
    "\n",
    "* Create a list with all the states;\n",
    "* Create a list with all the actions;\n",
    "* For each action, define a `numpy` array with the corresponding transition probabilities;\n",
    "* Define a `numpy`array with the costs. Make sure that:\n",
    "    * The costs lie in the interval [0, 1]\n",
    "    * The cost for standing in the princess's cell is minimal\n",
    "    * The cost for standing in the dragon's cell is maximal\n",
    "    * The costs for the intermediate cells are around 1/5 of those of standing in the dragon's cell\n",
    "\n",
    "The order for the states and actions used in the transition probability and cost matrices should match that in the lists of states and actions. \n",
    "\n",
    "**Note**: Don't forget to import `numpy`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "raw_mimetype": "text/x-python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: [1, 2, 3, 4, 5, 6]\n",
      "actions: ['up', 'down', 'left', 'right']\n",
      "Transition probabilities with action UP:\n",
      " [[0.8 0.1 0.  0.1 0.  0. ]\n",
      " [0.1 0.7 0.1 0.  0.1 0. ]\n",
      " [0.  0.1 0.8 0.  0.  0.1]\n",
      " [0.6 0.  0.  0.3 0.1 0. ]\n",
      " [0.  0.6 0.  0.1 0.2 0.1]\n",
      " [0.  0.  0.6 0.  0.1 0.3]]\n",
      "Transition probabilities with action DOWN:\n",
      " [[0.3 0.1 0.  0.6 0.  0. ]\n",
      " [0.1 0.2 0.1 0.  0.6 0. ]\n",
      " [0.  0.1 0.3 0.  0.  0.6]\n",
      " [0.1 0.  0.  0.8 0.1 0. ]\n",
      " [0.  0.1 0.  0.1 0.7 0.1]\n",
      " [0.  0.  0.1 0.  0.1 0.8]]\n",
      "Transition probabilities with action LEFT:\n",
      " [[0.8 0.1 0.  0.1 0.  0. ]\n",
      " [0.6 0.2 0.1 0.  0.1 0. ]\n",
      " [0.  0.6 0.3 0.  0.  0.1]\n",
      " [0.1 0.  0.  0.8 0.1 0. ]\n",
      " [0.  0.1 0.  0.6 0.2 0.1]\n",
      " [0.  0.  0.1 0.  0.6 0.3]]\n",
      "Transition probabilities with action RIGHT:\n",
      " [[0.3 0.6 0.  0.1 0.  0. ]\n",
      " [0.1 0.2 0.6 0.  0.1 0. ]\n",
      " [0.  0.1 0.8 0.  0.  0.1]\n",
      " [0.1 0.  0.  0.3 0.6 0. ]\n",
      " [0.  0.1 0.  0.1 0.2 0.6]\n",
      " [0.  0.  0.1 0.  0.1 0.8]]\n",
      "Costs:\n",
      " [[0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2]\n",
      " [1.  1.  1.  1. ]\n",
      " [0.  0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "gamma = 0.99\n",
    "states = [1,2,3,4,5,6]\n",
    "actions = ['up','down','left','right']\n",
    "\n",
    "PU = np.array([[0.8,0.1,0,0.1,0,0],\n",
    "               [0.1,0.7,0.1,0,0.1,0],\n",
    "               [0,0.1,0.8,0,0,0.1],\n",
    "               [0.6,0,0,0.3,0.1,0],\n",
    "               [0,0.6,0,0.1,0.2,0.1],\n",
    "               [0,0,0.6,0,0.1,0.3]]) \n",
    "\n",
    "PD = np.array([[0.3,0.1,0,0.6,0,0],\n",
    "               [0.1,0.2,0.1,0,0.6,0],\n",
    "               [0,0.1,0.3,0,0,0.6],\n",
    "               [0.1,0,0,0.8,0.1,0],\n",
    "               [0,0.1,0,0.1,0.7,0.1],\n",
    "               [0,0,0.1,0,0.1,0.8]])\n",
    "\n",
    "PL = np.array([[0.8,0.1,0,0.1,0,0],\n",
    "               [0.6,0.2,0.1,0,0.1,0],\n",
    "               [0,0.6,0.3,0,0,0.1],\n",
    "               [0.1,0,0,0.8,0.1,0],\n",
    "               [0,0.1,0,0.6,0.2,0.1],\n",
    "               [0,0,0.1,0,0.6,0.3]])\n",
    "\n",
    "PR = np.array([[0.3,0.6,0,0.1,0,0],\n",
    "               [0.1,0.2,0.6,0,0.1,0],\n",
    "               [0,0.1,0.8,0,0,0.1],\n",
    "               [0.1,0,0,0.3,0.6,0],\n",
    "               [0,0.1,0,0.1,0.2,0.6],\n",
    "               [0,0,0.1,0,0.1,0.8]])\n",
    "\n",
    "C = np.array([[0.2,0.2,0.2,0.2],\n",
    "              [0.2,0.2,0.2,0.2],\n",
    "              [0.2,0.2,0.2,0.2],\n",
    "              [0.2,0.2,0.2,0.2],\n",
    "              [1,1,1,1],\n",
    "              [0,0,0,0]])\n",
    "print('states:', states)\n",
    "print('actions:', actions)\n",
    "print('Transition probabilities with action UP:\\n', PU)\n",
    "print('Transition probabilities with action DOWN:\\n', PD)\n",
    "print('Transition probabilities with action LEFT:\\n', PL)\n",
    "print('Transition probabilities with action RIGHT:\\n', PR)\n",
    "print('Costs:\\n', C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prediction\n",
    "\n",
    "You are now going to evaluate a given policy, computing the corresponding cost-to-go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.\n",
    "\n",
    "Describe the policy that, in each state $x$, always moves the knight to the cell closest to the princess (irrespectively of the dragon). If multiple such cells exist, the knight should select randomly between the two.\n",
    "\n",
    "For example, suppose that the knight is in cell 1. The knight should then select randomly between the actions _D_ and _R_. Conversely, suppose that the knight is in cell 4. The knight should then select actions _R_ with probability 1.\n",
    "\n",
    "**Note:** The policy should be described as a vector with as many rows as there are states and as many columns as there are actions, where the entry _xa_ has the probability of selecting action _a_ in state _x_.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.5 0.  0.5]\n",
      " [0.  0.5 0.  0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  0.5 0.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "P = np.array([[0,0.5,0,0.5],\n",
    "              [0,0.5,0,0.5],\n",
    "              [0,1,0,0],\n",
    "              [0,0,0,1],\n",
    "              [0,0,0,1],\n",
    "              [0,0.5,0,0.5]])\n",
    "\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.\n",
    "\n",
    "Compute the cost-to-go function $J^\\pi$ associated with the policy from Activity 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.26056701 15.95826371 15.28584405 16.45495016 16.42766638 15.09441121]\n"
     ]
    }
   ],
   "source": [
    "cpi = np.diag(P[:,0]).dot(C[:,0]) + np.diag(P[:,1]).dot(C[:,1]) + np.diag(P[:,2]).dot(C[:,2]) + np.diag(P[:,3]).dot(C[:,3])\n",
    "Ppi = np.diag(P[:,0]).dot(PU) + np.diag(P[:,1]).dot(PD) + np.diag(P[:,2]).dot(PL) + np.diag(P[:,3]).dot(PR)\n",
    "J = np.linalg.inv(np.eye(6) - gamma * Ppi).dot(cpi)\n",
    "\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Control\n",
    "\n",
    "In this section you are going to compare value and policy iteration, both in terms of time and number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 4\n",
    "\n",
    "Show that the policy in Activity 3 is _not_ optimal: use value iteration to compute $J^*$ and show that $J^*\\neq J^\\pi$. Track the time and the number of iterations taken to compute $J^*$.\n",
    "\n",
    "**Note 1:** Stop the algorithm when the error between iterations is smaller than $10^{-8}$.\n",
    "\n",
    "**Note 2:** You may find useful the function ``time()`` from the module ``time``.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.06797074]\n",
      " [13.93809867]\n",
      " [13.67954678]\n",
      " [14.24800834]\n",
      " [14.74722471]\n",
      " [13.53004987]]\n",
      "time: 0.04946637153625488\n",
      "1726 iterations\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "gamma = 0.99\n",
    "c = np.array([[0.2],[0.2],[0.2],[0.2],[1],[0]])\n",
    "\n",
    "\n",
    "J = np.zeros((6,1))\n",
    "err = 1\n",
    "i = 0\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "while err > 1e-8:\n",
    "    QU = c + gamma * PU.dot(J)\n",
    "    QD = c + gamma * PD.dot(J)    \n",
    "    QL = c + gamma * PL.dot(J)    \n",
    "    QR = c + gamma * PR.dot(J)\n",
    "    Jnew = np.min((QU,QD,QL,QR), axis=0)\n",
    "    err = np.linalg.norm(Jnew-J)\n",
    "    i+=1\n",
    "    J = Jnew\n",
    "\n",
    "time = time.time() - time0\n",
    "    \n",
    "print(J)\n",
    "print('time:', time)\n",
    "print(i,'iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 5\n",
    "\n",
    "Compute once again the optimal policy now using policy iteration. Track the time and number of iterations taken and compare to those of Activity 4.\n",
    "\n",
    "**Note:** If you find that numerical errors affect your computations (especially when comparing two values/arrays) you may use the `numpy` function `isclose` with adequately set absolute and relative tolerance parameters (e.g., $10^{-8}$).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3 0.6 0.  0.1 0.  0. ]\n",
      " [0.1 0.2 0.6 0.  0.1 0. ]\n",
      " [0.  0.1 0.3 0.  0.  0.6]\n",
      " [0.6 0.  0.  0.3 0.1 0. ]\n",
      " [0.  0.1 0.  0.1 0.2 0.6]\n",
      " [0.  0.  0.1 0.  0.1 0.8]]\n",
      "[[0.  0.  0.  1. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  1.  0.  0. ]\n",
      " [1.  0.  0.  0. ]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  0.5 0.  0.5]]\n",
      "time:  0.01357889175415039\n",
      "4 iterations\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "pi = np.ones((6,4)) / 2\n",
    "quit = False\n",
    "i=0\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "while not quit:\n",
    "    cpi = np.diag(pi[:,0]).dot(C[:,0]) + np.diag(pi[:,1]).dot(C[:,1]) + np.diag(pi[:,2]).dot(C[:,2]) + np.diag(pi[:,3]).dot(C[:,3])\n",
    "    Ppi = np.diag(pi[:,0]).dot(PU) + np.diag(pi[:,1]).dot(PD) + np.diag(pi[:,2]).dot(PL) + np.diag(pi[:,3]).dot(PR)\n",
    "    J = np.linalg.inv(np.eye(6) - gamma * Ppi).dot(cpi)\n",
    "    \n",
    "    QU = C[:,0] + gamma * PU.dot(J)\n",
    "    QD = C[:,1] + gamma * PD.dot(J)\n",
    "    QL = C[:,2] + gamma * PL.dot(J)\n",
    "    QR = C[:,3] + gamma * PR.dot(J)\n",
    "    \n",
    "    pinew = np.zeros((6,4))\n",
    "    pinew[:,0] = np.isclose(QU,np.min([QU,QD,QL,QR], axis=0), atol=1e-8,rtol=1e-8).astype(int)\n",
    "    pinew[:,1] = np.isclose(QD,np.min([QU,QD,QL,QR], axis=0), atol=1e-8,rtol=1e-8).astype(int)\n",
    "    pinew[:,2] = np.isclose(QL,np.min([QU,QD,QL,QR], axis=0), atol=1e-8,rtol=1e-8).astype(int)\n",
    "    pinew[:,3] = np.isclose(QR,np.min([QU,QD,QL,QR], axis=0), atol=1e-8,rtol=1e-8).astype(int)\n",
    "    \n",
    "    pinew = pinew/np.sum(pinew, axis=1, keepdims=True)\n",
    "    \n",
    "    quit = (pi == pinew).all()\n",
    "    pi = pinew\n",
    "    i += 1\n",
    "\n",
    "time = time.time() - time0\n",
    "\n",
    "print(Ppi)\n",
    "print(pi)\n",
    "print('time: ', time)\n",
    "print(i, 'iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Simulation\n",
    "\n",
    "Finally, in this section you will check whether the theoretical computations of the cost-to-go actually correspond to the cost incurred by an agent following a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 6\n",
    "\n",
    "Starting both in cell 1 and cell 5 in the figure, \n",
    "\n",
    "* Generate **100** trajectories of 10,000 steps each, following the optimal policy for the MDP. \n",
    "* For each trajectory, compute the accumulated (discounted) cost. \n",
    "* Compute the average cost over the 100 trajectories.\n",
    "* Compare the resulting value with that computed in Activity 4 for the two states. \n",
    "\n",
    "** Note:** The simulation may take a bit of time, don't despair ☺️.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.55159158]\n",
      "[13.40646352]\n",
      "[15.44399952]\n",
      "[16.02201052]\n",
      "[19.86223895]\n",
      "[11.58139069]\n",
      "[12.52111048]\n",
      "[12.83181974]\n",
      "[12.37736268]\n",
      "[12.76509005]\n",
      "[11.08552249]\n",
      "[11.7209639]\n",
      "[11.50228528]\n",
      "[16.97914631]\n",
      "[14.8370952]\n",
      "[16.32069849]\n",
      "[12.01617007]\n",
      "[13.28339729]\n",
      "[16.34170951]\n",
      "[15.75545226]\n",
      "[12.00616401]\n",
      "[16.39147996]\n",
      "[17.0008346]\n",
      "[14.35234214]\n",
      "[11.31783677]\n",
      "[17.85310365]\n",
      "[18.23467141]\n",
      "[17.28963409]\n",
      "[13.76359732]\n",
      "[13.26173685]\n",
      "[17.24287539]\n",
      "[14.22979736]\n",
      "[12.37529346]\n",
      "[12.20886072]\n",
      "[9.12377411]\n",
      "[12.62317016]\n",
      "[12.83816731]\n",
      "[8.88887338]\n",
      "[15.54991431]\n",
      "[16.26717346]\n",
      "[16.31043617]\n",
      "[16.58296797]\n",
      "[15.17728685]\n",
      "[11.18039112]\n",
      "[12.55914395]\n",
      "[12.91856199]\n",
      "[13.37432211]\n",
      "[19.023165]\n",
      "[10.18706892]\n",
      "[17.64393517]\n",
      "[11.8277175]\n",
      "[17.70154734]\n",
      "[11.52288607]\n",
      "[15.06358478]\n",
      "[11.31281158]\n",
      "[14.26334509]\n",
      "[16.09084674]\n",
      "[13.6482082]\n",
      "[9.95773585]\n",
      "[12.60550909]\n",
      "[11.44460976]\n",
      "[16.35976283]\n",
      "[13.83499994]\n",
      "[12.31593929]\n",
      "[11.69490821]\n",
      "[9.69196874]\n",
      "[13.02816828]\n",
      "[19.48208873]\n",
      "[10.904936]\n",
      "[11.61043264]\n",
      "[15.52346739]\n",
      "[15.23301397]\n",
      "[12.58458476]\n",
      "[8.98660766]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3699d820b0be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "costs = np.empty(100)\n",
    "\n",
    "for j in range(100):\n",
    "    steps = np.empty(10000)\n",
    "    pos = 1\n",
    "    cost = 0\n",
    "    for i in range(9999):\n",
    "        move = np.random.choice(np.arange(0, 4), p=pi[pos-1])\n",
    "        if move==0:\n",
    "            pos = np.random.choice(np.arange(0, 6), p=PU[pos-1])+1\n",
    "        elif move==1:\n",
    "            pos = np.random.choice(np.arange(0, 6), p=PD[pos-1])+1\n",
    "        elif move==2:\n",
    "            pos = np.random.choice(np.arange(0, 6), p=PL[pos-1])+1\n",
    "        elif move==3:\n",
    "            pos = np.random.choice(np.arange(0, 6), p=PR[pos-1])+1\n",
    "        steps[i] = pos \n",
    "        cost += c[pos-1]*(gamma**i)\n",
    "    costs[j] = cost\n",
    "    print(cost)\n",
    "print('average cost over the 100 trajectories, starting in position 1:', np.sum(costs)/100)\n",
    "\n",
    "costs = np.empty(100)\n",
    "for j in range(100):\n",
    "    steps = np.empty(10000)\n",
    "    pos = 5\n",
    "    cost = 0\n",
    "    for i in range(9999):\n",
    "        move = np.random.choice(np.arange(0, 4), p=pi[pos-1])\n",
    "        #UP\n",
    "        if move==0: \n",
    "            pos = np.random.choice(np.arange(0, 6), p=PU[pos-1])+1\n",
    "        #DOWN\n",
    "        elif move==1: \n",
    "            pos = np.random.choice(np.arange(0, 6), p=PD[pos-1])+1\n",
    "        #LEFT\n",
    "        elif move==2:\n",
    "            pos = np.random.choice(np.arange(0, 6), p=PL[pos-1])+1\n",
    "        #RIGHT\n",
    "        elif move==3:\n",
    "            pos = np.random.choice(np.arange(0, 6), p=PR[pos-1])+1\n",
    "        steps[i] = pos \n",
    "        cost += c[pos-1]*(gamma**i)\n",
    "    costs[j] = cost\n",
    "    print(cost)\n",
    "print('average cost over the 100 trajectories, starting in position 5:', np.sum(costs)/100)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average cost over the 100 trajectories starting in cell 1 should be lower than the average cost over the 100 trajectories starting in cell 5, as expected from the costs obtained through value iteration (activity 4).\n",
    "\n",
    "For us, this doesn't happen everytime we run the code, even though our logic is right the calculations done could be rounding in a way that justifies the ocasional wriong results we gor."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
